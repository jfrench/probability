---
format:
    revealjs:
        css: style.css
self-contained: true
callout-icon: false
callout-appearance: simple
---

# 1.5 Independence

## Definition

::: {.callout-note}
**Independent Events**
Events $A$ and $B$ are **independent** if

$$
P(A\cap B) = P(A)P(B).
$$
:::

Equivalently, $A$ and $B$ are independent if

$$
P(A \mid B) = P(A),
$$

assuming $P(B) > 0$.

The first definition is preferred because it can be used even when $P(A \mid B)$ is undefined.

## Implications

Practically, two events are independent if knowledge about one of the events occurring has no impact the probability of the other event occurring.

If $A$ is independent of $B$, then $B$ is independent of $A$.

Shockingly, disjoint events cannot be independent!

Why? If $P(A) > 0$ and $P(B) > 0$ but $A$ and $B$ are disjoint, then
$$
0 = P(A \cap  B) \neq P(A)P(B) > 0.
$$

## Example 1.19

Consider an experiment involving two successive rolls of a 4-sided die in which all 16 possible outcomes are equally likely and have probability 1/16.

## Example 1.19 (cont)

Are the events below independent?
$$
A_i = \{1\text{st roll results in }i\}, \quad
B_j = \{2\text{nd roll results in }j\}
$$

## Example 1.19 (cont)

Are the events below independent?
$$
A_i = \{1\text{st roll is a }1\}, \quad
B_j = \{\text{sum of the two rolls is }5\}
$$

## Example 1.19 (continued)

Are the events below independent?
$$
A_i = \{\text{maximum of the two rolls is }2\},
$$
$$
B_j = \{\text{minimum of the two rolls is }2\}
$$

# Conditional Independence

## Definition

::: {.callout-note}

**Conditionally Indpendent Events**

Given an event $C$, the events $A$ and $B$ are **conditionally indpendent** if 
$$
P(A\cap B \mid C) = P(A \mid C) P(B \mid C).
$$
:::

The independence of $A$ and $B$ does not imply the conditional independence of the events.

## Alternative Characterization

## Example 1.20

Consider two independent fair coin tosses, in which all four possible outcomes are equally likely. Let 
$$
\{H_1\} = 1\text{st toss is a head},
$$
$$\{H_2\} = 2\text{nd toss is a head},
$$
$$
D = \{\text{the two tosses have different results}\}.
$$

## Example 1.20 (cont)

## Example 1.21

There are two coins, a blue and red one. We choose one of the two coins at random, each being chosen with probability $1/2$. We proceed with two independent oses. The coins are biased: with the blue coin, the probability of heads in any given toss is 0.99, whereas for the red coin it is 0.01. 

Let $B$ be the event that the blue coin was selected. Let $H_i$ be teh event that the $i$th toss results in a head. 

The events $H_1$ and $H_2$ are dependent. However, they are conditionally independent given the coin.

## Example 1.21 (cont)

## Summary

::: {.callout-note}
- Two event $A$ and $B$ are **independent** if $P(A \cap B) = P(A) P(B)$.
- If $A$ and $B$ are independent, then $A$ and $B^c$ are independent.
- Two events $A$ and $B$ are **conditionally independent** given another event $C$ with $P(C) > 0$, if $P(A \cap B \mid C) = P(A \mid C) P(B \mid C)$.
- Independence does not imply conditional independence or vice versa.
:::

# Independence of a Collection of Events

## Independence Generalization
::: {.callout-note}
**Definition of Independence of Several Events**

The events $A_1, A_2, \ldots, A_n$ are independent if
$$
P\left(\cap_{i\in S} A_i\right) = \prod_{i\in S} P(A_i),\quad\text{for every subset}S\subseteq \{1,2,\ldots,n\}.
$$
:::

## Independence Generalization

If $A_1, A_2, \text{ and } A_3$ are events, what properties do they have to satisfy to be independent?

## Example 1.22

Pairwise independence does not imply independence. 

Consider two independent fair coin tosses, in which all four possible outcomes are equally likely. Let
$$
\{H_1\} = 1\text{st toss is a head},
$$
$$\{H_2\} = 2\text{nd toss is a head},
$$
$$
D = \{\text{the two tosses have different results}\}.
$$

## Example 1.22 (cont)

## Example 1.23

The equality $P(A_1 \cap A_2 \cap A_3) = P(A_1)P(A_2)P(A_3)$ does not guarantee independence.

Consider two rolls of a fair six-sided die, and the following events:

$$
A = \{1\text{st roll is}1, 2, \text{ or }3\},
$$
$$
B = \{1\text{st roll is}3, 4, \text{ or }5\},
$$
$$
C = \{\text{the sum of the two rolls is }9\}.
$$

## Example 1.23 (cont)

## Intuition

Independence means that that the occurrence or non-occurrence of **any number** of the events from that collection carries no information on the remaining events or their complements.

# Reliability

## Context

In probabiliistic models of complex systems involving several components, it is often convenient (and intelligent) to assume that the behaviors of the components are uncoupled (independent).

## Example 1.24 Network Connective

A computer network connects two nodes $A$ and $B$ through intermediate nodes $C, D, E, F$.

For every pair of directly connected nodes, say $i$ and $j$, there is a given probability $p_{ij}$ that the link from $i$ to $j$ is functioning properly. We assume that link failures are independent of each other.

What is the probability that there is a path connecting $A$ and $B$ in which all links are up?

## Example 1.24 (cont)

![Reliability example](/images/f1-15.png){fig-alt="Network diagram for reliability example."}

## Example 1.24 (cont)

# Independent Trials and the Binomial Probabilities

## Context

If an experiment involves are sequence of identical smaller experiements, then we have a sequence of **independent trials**.

If each trial has only two possible outcomes, then we have a sequence of **Bernoulli trials**.

Assume we flip a coin three times, successively and independently. Let $H_i$ denote flipping a head on trial $i$ and $P(H_i) = p$.

## Bernoulli trials visualized

![Bernoulli trials tree diagram](/images/f1-16.png){fig-alt="Bernoulli trials tree diagram."}

## Binomial probabilities

For a set of Bernoulli trials, consider the probability

$$
p(k) = P(k\text{ heads come up in an }n\text{-toss sequence}).
$$

The probability for a particular sequence (one branch of the tree) is 

Thus, 
$$
p(k) = 
$$

## Binomial probabilities

The numbers $\binom{n}{k}$ is read as "$n$ choose $k$" and is the number of distinct $n$-toss sequences that contain $k$ heads.

The numbers $\binom{n}{k}$ are known as the **binomial coefficients**.

The numbers $p(k)$ are known as the **binomial probabilities**.

## Example 1.25 Grade of Service

An internet service provider has installed $c$ modems to serve the needs of a population of $n$ dial-up customers.

Each customer will need a connection with probability $p$ independent of the other customers.

What is the probability that there are more customers needing a connection than modems?

Answer this problem generally, and then for $n = 100$, $p=0.1$, and $c=15$. 

## Example 1.25 (cont)